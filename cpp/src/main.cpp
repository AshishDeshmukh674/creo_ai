#include "NL2Trail.h"
#include <iostream>
#include <string>
#include <cstdio>
#include <windows.h>
#include <sstream>

// Helper function to get voice input from Python script
std::string get_voice_input() {
    // Get the directory of the running executable
    char exePath[MAX_PATH];
    GetModuleFileNameA(NULL, exePath, MAX_PATH);
    std::string exeDir(exePath);
    exeDir = exeDir.substr(0, exeDir.find_last_of("\\/"));

    // Build the path to the Python script
    std::stringstream cmd;
    cmd << "python \"" << exeDir << "\\..\\..\\..\\py\\voice_input.py\"";

    std::string result;
    FILE* pipe = _popen(cmd.str().c_str(), "r");
    if (!pipe) return "ERROR: Could not open pipe.";
    char buffer[256];
    while (fgets(buffer, sizeof(buffer), pipe) != nullptr) {
        result += buffer;
    }
    _pclose(pipe);
    // Remove trailing newlines
    while (!result.empty() && (result.back() == '\n' || result.back() == '\r')) result.pop_back();
    return result;
}




int main(int argc, char* argv[]) {
    try {
        // ===== Configuration =====
        // Paths to the exported ONNX model and tokenizer files
        // These should be generated by running the Python export script
        std::string onnx_path = "../../onnx_model/t5_creo.onnx";     // ONNX model file
        std::string spm_path  = "../../onnx_model/spiece.model";    // SentencePiece tokenizer

        // ===== Model Initialization =====
        std::cout << "Loading NL2Trail model..." << std::endl;
        std::cout << "ONNX Model: " << onnx_path << std::endl;
        std::cout << "Tokenizer:  " << spm_path << std::endl;

        // Initialize the NL2Trail inference engine
        // maxNewTokens=256 limits the maximum length of generated trail commands
        NL2Trail model(onnx_path, spm_path, /*maxNewTokens=*/256);
        
        std::cout << "Model loaded successfully!" << std::endl << std::endl;

        // ===== Input Handling =====
        std::string nl;
        
        if (argc > 1) {
            // Use command line argument if provided
            nl = argv[1];
            std::cout << "Using command line input: " << nl << std::endl;
        } else {
            // Interactive input mode
            std::cout << "=== NL2Trail Interactive Mode ===" << std::endl;
            std::cout << "Choose input mode:\n1. Text\n2. Voice\nEnter 1 or 2: ";
            int choice = 1;
            std::cin >> choice;
            std::cin.ignore(); // clear newline
            if (choice == 2) {
                nl = get_voice_input();
                if (nl.rfind("ERROR", 0) == 0) {
                    std::cout << "Voice input failed. Falling back to text input.\nEnter natural language description: ";
                    std::getline(std::cin, nl);
                } else {
                    std::cout << "You said: " << nl << std::endl;
                }
            } else {
                std::cout << "Enter natural language description (or 'quit' to exit):" << std::endl;
                std::cout << "> ";
                std::getline(std::cin, nl);
            }
            if (nl == "quit" || nl == "exit") {
                std::cout << "Goodbye!" << std::endl;
                return 0;
            }
        }

        // if (nl.empty()) {
        //     // Use default example if no input provided
        //     nl = "Create a new solid cube with side length 50 mm using rectangle sketch and extrude.";
        //     std::cout << "Using default example input." << std::endl;
        // }
        
        std::cout << std::endl << "Input Natural Language:" << std::endl;
        std::cout << "----------------------" << std::endl;
        std::cout << nl << std::endl << std::endl;

        // Generate Creo trail commands from the natural language input
        std::cout << "Generating Creo trail commands..." << std::endl;
        std::string trail = model.generate(nl);

        // ===== Output Results =====
        std::cout << "Generated Creo Trail Commands:" << std::endl;
        std::cout << "=============================" << std::endl;
        std::cout << trail << std::endl;
        std::cout << "=============================" << std::endl;

    } catch (const std::exception& ex) {
        // Handle any errors that occur during model loading or inference
        std::cerr << "FATAL ERROR: " << ex.what() << std::endl;
        std::cerr << std::endl;
        std::cerr << "Troubleshooting tips:" << std::endl;
        std::cerr << "1. Ensure ONNX Runtime is properly installed" << std::endl;
        std::cerr << "2. Verify that the model files exist and are valid" << std::endl;
        std::cerr << "3. Check that SentencePiece library is available" << std::endl;
        std::cerr << "4. Make sure the model was exported correctly from Python" << std::endl;
        return 1;
    }

    return 0;
}
